{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Char_RNN_Task_2 (1).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/figo721/AI_LIFE_SCIENCE/blob/main/Char_RNN_Task_2_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02PM9_GczMbi"
      },
      "source": [
        ""
      ],
      "id": "02PM9_GczMbi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TuSynPLzNzh",
        "outputId": "f8eef10e-5d19-4892-cf3f-4f8b2affc366"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "6TuSynPLzNzh",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "parallel-dubai"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "parallel-dubai",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dependent-madness"
      },
      "source": [
        "https://colab.research.google.com/github/abhaysrivastav/ComputerVision/blob/master/Chararacter_Level_RNN.ipynb\n",
        " https://tracyrenee61.medium.com/code-review-character-level-lstm-in-pytorch-b1ef188c8769   \n",
        "    https://blog.bayeslabs.co/2019/07/04/Generating-Molecules-using-Char-RNN-in-Pytorch.html"
      ],
      "id": "dependent-madness",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fitting-victorian",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7c2e4ca0-b7c1-4105-8133-82b9befbe092"
      },
      "source": [
        ""
      ],
      "id": "fitting-victorian",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fd7b90d5aa4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmiles_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mint2char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchar2int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mint2char\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Encode the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar2int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmiles_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'smiles_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aggregate-german"
      },
      "source": [
        "with open('./drive/MyDrive/AI_LIFE/smiles_train_2021.txt', \"r\") as file:\n",
        "    my_list = [smile.rstrip(\"\\n\") for smile in file]\n",
        "    \n",
        "my_list_pd = pd.DataFrame(my_list)\n",
        "my_list_pd.columns = ['smiles']\n",
        "\n",
        "\n",
        "smiles_train, smiles_test = train_test_split(my_list_pd[\"smiles\"], test_size = 0.30,random_state=42)\n",
        "\n",
        "smiles_train = pd.DataFrame(smiles_train)\n",
        "smiles_test = pd.DataFrame(smiles_test)"
      ],
      "id": "aggregate-german",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4_MKCxJzutG"
      },
      "source": [
        "chars = tuple(set(smiles_train))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "# Encode the text\n",
        "encoded = np.array([char2int[ch] for ch in smiles_train])"
      ],
      "id": "p4_MKCxJzutG",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "activated-infection"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array\n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "id": "activated-infection",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fluid-pavilion"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # total number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batches * batch_size_total]\n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    \n",
        "    # iterate through the array, one sequence at a time\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "id": "fluid-pavilion",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "demographic-generation"
      },
      "source": [
        "batches = get_batches(encoded,10,50)"
      ],
      "id": "demographic-generation",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "known-bunny",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "75b1dabc-e6ca-4590-ad38-2310609f74bd"
      },
      "source": [
        "x,y = next(batches)"
      ],
      "id": "known-bunny",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e7c70fd29eb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opponent-telephone",
        "outputId": "12aa28c0-fb3b-4a4d-8bbb-010f6cdc44f8"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "id": "opponent-telephone",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "industrial-muslim"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.6, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "        \n",
        "        # initialize the weights\n",
        "        self.init_weights()\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hc):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hc`. '''\n",
        "        \n",
        "        ## TODO: Get x, and the new hidden state (h, c) from the lstm\n",
        "        x, (h, c) = self.lstm(x, hc)\n",
        "        \n",
        "        ## TODO: pass x through a droupout layer\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        x = x.reshape(x.size()[0]*x.size()[1], self.n_hidden)\n",
        "        \n",
        "        ## TODO: put x through the fully-connected layer\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        # return x and the hidden state (h, c)\n",
        "        return x, (h, c)\n",
        "    \n",
        "    \n",
        "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "        \n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        if cuda:\n",
        "            self.cuda()\n",
        "        else:\n",
        "            self.cpu()\n",
        "        \n",
        "        if h is None:\n",
        "            h = self.init_hidden(1)\n",
        "        \n",
        "        x = np.array([[self.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(self.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        if cuda:\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        h = tuple([each.data for each in h])\n",
        "        out, h = self.forward(inputs, h)\n",
        "\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if cuda:\n",
        "            p = p.cpu()\n",
        "        \n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(self.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "            \n",
        "        return self.int2char[char], h\n",
        "    \n",
        "    def init_weights(self):\n",
        "        ''' Initialize weights for fully connected layer '''\n",
        "        initrange = 0.1\n",
        "        \n",
        "        # Set bias tensor to all zeros\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        # FC weights as random uniform\n",
        "        self.fc.weight.data.uniform_(-1, 1)\n",
        "        \n",
        "    def init_hidden(self, n_seqs):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
        "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())\n",
        "        "
      ],
      "id": "industrial-muslim",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "typical-belarus"
      },
      "source": [
        "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.01, clip=5, val_frac=0.25, cuda=True, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
        "        n_steps: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        cuda: Train with CUDA on a GPU\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    net.train()\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if cuda:\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        h = net.init_hidden(n_seqs)\n",
        "        for x, y in get_batches(data, n_seqs, n_steps):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            net.zero_grad()\n",
        "            \n",
        "            output, h = net.forward(inputs, h)\n",
        "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
        "\n",
        "            loss.backward()\n",
        "            \n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "\n",
        "            opt.step()\n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "                \n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(n_seqs)\n",
        "                val_losses = []\n",
        "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if cuda:\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net.forward(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "id": "typical-belarus",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "british-illustration",
        "outputId": "8e251548-624e-4573-edba-ed6b4fb5b7cc"
      },
      "source": [
        "net = CharRNN(chars, n_hidden=256, n_layers=4)\n",
        "print(net)"
      ],
      "id": "british-illustration",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(1, 256, num_layers=4, batch_first=True, dropout=0.6)\n",
            "  (dropout): Dropout(p=0.6, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sensitive-salem"
      },
      "source": [
        "n_seqs, n_steps = 128, 100\n",
        "\n",
        "# you may change cuda to True if you plan on using a GPU!\n",
        "# also, if you do, please INCREASE the epochs to 25\n",
        "train(net, encoded, epochs=3, n_seqs=n_seqs, n_steps=n_steps, lr=0.01, cuda=True, print_every=10)"
      ],
      "id": "sensitive-salem",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amateur-round"
      },
      "source": [
        ""
      ],
      "id": "amateur-round",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "conceptual-pressure"
      },
      "source": [
        ""
      ],
      "id": "conceptual-pressure",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "willing-priest"
      },
      "source": [
        "def sample(net, size, prime='The', top_k=8):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "    \n",
        "    return ''.join(chars)\n",
        "    \n",
        "# Generating new text\n",
        "print(sample(net, 120, prime='A', top_k=5))"
      ],
      "id": "willing-priest",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sharp-guidance"
      },
      "source": [
        ""
      ],
      "id": "sharp-guidance",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "outer-serum"
      },
      "source": [
        ""
      ],
      "id": "outer-serum",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "announced-heritage",
        "outputId": "e0a25261-db29-4dc5-af61-487f5bb7633b"
      },
      "source": [
        "n_hidden=128\n",
        "n_layers=3 \n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net) \n",
        "# Declaring the hyperparameters\n",
        "batch_size = 32\n",
        "seq_length = 50\n",
        "n_epochs = 20 \n",
        "# start smaller if you are just testing initial behavior \n",
        "train the modeltrain(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001,print_every=10000)"
      ],
      "id": "announced-heritage",
      "execution_count": null,
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-20-2cc4421aa2a1>, line 10)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-2cc4421aa2a1>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    train the modeltrain(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001,print_every=10000)\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flush-parker"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "# get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "id": "flush-parker",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sublime-infection"
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "return ''.join(chars)\n",
        "    \n",
        "# Generating new text\n",
        "print(sample(net, 120, prime='A', top_k=5))"
      ],
      "id": "sublime-infection",
      "execution_count": null,
      "outputs": []
    }
  ]
}